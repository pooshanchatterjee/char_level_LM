# Character Level Language Modelling
Two character-level language models were constructed and
trained from scratch using a dataset comprising 32,000
distinct names. The primary objective was to generate novel
names.
The implementation of the Bigram models involved two
distinct approaches:
1. The explicit computation and utilisation of the bigram
counts matrix.
2. The construction of a Neural Network architecture
consisting of a single linear layer.
For the Trigram model, a Multi-Layered Perceptron was
employed, featuring an embedding layer, nonlinear hidden
layers utilizing tanh activation functions, and a final SoftMax
layer as the output. The dimensions of the embedding space
and the number of nodes in the hidden tanh layer were
predetermined during hyperparameter tuning.
In the concluding phase, both models were utilised to
generate fresh and distinctive names. Remarkably, the
samples generated by the Trigram model exhibited a more
name-like quality compared to their bigram counterparts.
